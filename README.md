# llmsonmobile
Mobile-Native GenAI: LLMs, Vector Search & RAG at the Edge

This project explores running modern GenAI workloads directly on mobile devices (iOS & Android) â€” including LLM inference, vector database search, and RAG â€” as a scalable alternative to centralized AI servers.

ğŸš€ Motivation

Most GenAI systems face a concurrency scalability problem:

LLMs typically generate 100â€“200 tokens/sec per request

Scaling to hundreds or thousands of users requires massive server resources

Cloud dependency introduces cost, latency, and data-control concerns

Our approach:
ğŸ‘‰ Push inference and retrieval to the edge using mobile devices.

ğŸ“± What This Project Does

Runs LLMs natively on iOS and Android

Implements mobile-native vector databases & search

Enables on-device RAG pipelines

Provides a custom Python runtime for mobile AI workflows

This turns phones into distributed AI nodes, offloading concurrency from servers.

âœ… Current Capabilities

LLMs supported (1Bâ€“7B parameters):

DeepSeek

Phi-3

Mistral

Gemma

LLaMA

Qwen

Dolphin

Performance

Up to ~100 tokens/sec (device/model dependent)

Fast native vector search on iOS & Android

Platform Support

iOS (Metal / CoreML where applicable)

Android (CPU / NNAPI paths)

Python runtime on mobile

ğŸ§© Use Cases

Offline GenAI applications

Privacy-preserving inference

Edge RAG systems

Field visualization & analytics

Distributed AI concurrency

Cost-sensitive AI deployments

ğŸ› ï¸ Project Status

âœ… LLM inference on mobile

âœ… Vector DB + search

ğŸš§ RAG pipelines

ğŸš§ Developer SDKs

ğŸš§ Packaging & docs

See the Roadmap
 for details.

ğŸ¤ Who This Is For

Mobile developers

AI engineers

Edge computing researchers

Startups building privacy-first AI

Anyone interested in on-device GenAI

ğŸ“¬ Get Involved

If youâ€™re interested in:

Collaborating

Testing on new devices

Porting models

Building real-world apps

ğŸ‘‰ Open an issue, submit a PR, or connect.

Letâ€™s push the boundaries of mobile-native AI.
