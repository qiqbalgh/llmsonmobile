# llmsonmobile
This project explores running modern GenAI workloads directly on mobile devices (iOS &amp; Android) — including LLM inference, vector database search, and RAG — as a scalable alternative to centralized AI servers.
