# llmsonmobile
ðŸ“± Mobile-Native GenAI at the Edge

Running LLMs, Vector Search, and RAG directly on iOS & Android

Solving GenAI concurrency and data-control challenges by pushing inference to the edge.

Why This Project Exists

Modern GenAI systems face a fundamental scalability issue:

LLMs typically generate ~100â€“200 tokens/sec per request

Handling hundreds or thousands of concurrent users requires expensive servers

Cloud dependency increases cost, latency, and data exposure

This project explores a different approach:

Distribute GenAI workloads across mobile devices instead of scaling servers.

What This Enables

âœ” Native LLM inference on iOS & Android
âœ” Mobile-native vector database & similarity search
âœ” On-device RAG pipelines
âœ” A custom Python runtime for mobile AI workflows

Together, these turn smartphones into distributed AI edge nodes.

Current Capabilities
ðŸ¤– Model Support

1B â€“ 7B parameter models

Quantized & mixed-precision execution

Tested models include:

DeepSeek

Phi-3

Mistral

Gemma

LLaMA

Qwen

Dolphin

ðŸ“Š Example Benchmark

4M vectors Â· 100 dimensions Â· 5-NN query

Device	Index Build	Query Time
iPhone 16 Pro	674 ms	75 ms
Moto G (~$50)	8.66 s	281 Âµs

Even low-cost Android devices deliver exceptional query latency once indexed.

Supported Platforms
Platform	Status
iOS	âœ… Active
Android	âœ… Active
Python (mobile)	âœ… Working
RAG	ðŸš§ In progress
